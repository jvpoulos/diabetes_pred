sweep: false
use_labs: true
do_overwrite: false  
seed: 42
save_dir: "./experiments/finetune/${now:%Y-%m-%d_%H-%M-%S}"
dataset_path: null
pretrained_weights_fp: null
config:
  _target_: EventStream.transformer.config.StructuredTransformerConfig
  output_scores: false
  use_gradient_checkpointing: false
  use_cache: true # must be false if use_gradient_checkpointing is true
  use_labs: true
  use_batch_norm: false
  use_layer_norm: true
  use_flash_attention: true  # Set to false if you don't want to use Flash Attention
  problem_type: "single_label_classification"
  num_labels: 2
  do_use_sinusoidal: true # When True, it uses the LearnableFrequencySinusoidalTemporalPositionEncoding; When False, it uses the standard TemporalPositionEncoding
  do_split_embeddings: true
  categorical_embedding_dim: 32
  numerical_embedding_dim: 128
  static_embedding_mode: sum_all
  categorical_embedding_weight: 0.5
  numerical_embedding_weight: 0.7
  static_embedding_weight: 0.6
  dynamic_embedding_weight: 0.5
  do_normalize_by_measurement_index: false
  structured_event_processing_mode: conditionally_independent
  num_hidden_layers: 6
  seq_attention_types: ["global", "local"]
  seq_window_size: 168
  head_dim: 32
  num_attention_heads: 8
  max_grad_norm: 1
  intermediate_dropout: 0.1
  attention_dropout: 0.1
  input_dropout: 0.1
  resid_dropout: 0.1
  hidden_size: 256  # 32*8 = 256, consistent with head_dim and num_attention_heads
  intermediate_size: 512
  layer_norm_epsilon: 0.00001
  task_specific_params:
    pooling_method: mean

optimization_config:
  init_lr: 0.01  # Initial learning rate
  end_lr: 0.000001   # Final learning rate
  end_lr_frac_of_init_lr: 0.0001  # This should be equal to end_lr / init_lr
  max_epochs: 300 
  batch_size: 512
  validation_batch_size: 512
  lr_frac_warmup_steps: 0.05
  lr_decay_power: 0.05
  weight_decay: 0.01
  clip_grad_value: 1  # Default value for grad_value clipping
  use_grad_value_clipping: true  # Flag to enable grad_value clipping
  patience: 5
  lr_num_warmup_steps: 200  
  max_training_steps: 316500 # steps per epoch = training samples/effective batch size
  use_lr_scheduler: true
  lr_scheduler_type: "linear"  # can be "cosine", "linear", "one_cycle", "reduce_on_plateau", or "null"
  num_dataloader_workers: 0
  
data_config:
  save_dir: "./data/labs"
  dl_reps_dir: "data/labs/DL_reps"
  dataset_path: null
  max_seq_len: 256
  subsequence_sampling_strategy: to_end
  min_seq_len: 4
  train_subset_size: "FULL"
  train_subset_seed: null
  task_df_name: "a1c_greater_than_7"
  seq_padding_side: right
  do_include_subject_id: False
  do_include_start_time_min: False
  do_include_subsequence_indices: False

trainer_config:
  accelerator: auto
  devices: 1
  precision: "16-mixed"
  detect_anomaly: false
  log_every_n_steps: 100
  strategy: "ddp_find_unused_parameters_true"
  accumulate_grad_batches: 4

experiment_dir: "./experiments"

wandb_logger_kwargs:
  name: "transformer_labs"
  project: "diabetes_finetune_labs"
  team: null
  log_model: false
  do_log_graph: false

wandb_experiment_config_kwargs:
  entity: "jvpoulos"
  project: "diabetes_finetune_labs"
  team: null
  log_model: false
  do_log_graph: false
  
do_final_validation_on_metrics: false
do_use_filesystem_sharing: false
