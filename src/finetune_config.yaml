sweep: false
use_labs: true
do_overwrite: false  
seed: 42
save_dir: "./experiments/finetune/${now:%Y-%m-%d_%H-%M-%S}"
dataset_path: null
pretrained_weights_fp: null
config:
  _target_: EventStream.transformer.config.StructuredTransformerConfig
  use_fake_feature: true
  fake_feature_correlation: 0.8
  use_gradient_checkpointing: false
  use_cache: true # must be false if use_gradient_checkpointing is true
  use_labs: true
  use_static_features: true
  use_batch_norm: false
  use_layer_norm: true
  use_flash_attention: true
  attention_mechanism: "stable_attention" # (original_improved or stable_attention)
  output_attentions: true
  problem_type: "single_label_classification"
  num_labels: 2
  do_use_sinusoidal: true # When True, it uses the LearnableFrequencySinusoidalTemporalPositionEncoding; When False, it uses the standard TemporalPositionEncoding
  do_split_embeddings: true
  categorical_embedding_dim: 32
  numerical_embedding_dim: 16
  static_embedding_mode: sum_all
  categorical_embedding_weight: 0.3
  numerical_embedding_weight: 0.7
  static_embedding_weight: 0.7
  dynamic_embedding_weight: 0.5
  do_normalize_by_measurement_index: false
  structured_event_processing_mode: conditionally_independent
  num_hidden_layers: 2
  seq_attention_types: ["global", "local"]
  seq_window_size: 168 # An integer specifying the size of the attention window.
  head_dim: 64
  max_position_embeddings: 512
  num_attention_heads: 1
  max_grad_norm: 1
  intermediate_dropout: 0.1
  attention_dropout: 0.1
  input_dropout: 0.1
  resid_dropout: 0.1
  hidden_size: 64  # consistent with head_dim and num_attention_heads
  intermediate_size: 128
  layer_norm_epsilon: 1e-3
  task_specific_params:
    pooling_method: mean

optimization_config:
  init_lr: 0.001  # Initial learning rate
  end_lr: 0.0001   # Final learning rate
  end_lr_frac_of_init_lr: 0.1  # This should be equal to end_lr / init_lr
  max_epochs: 100
  batch_size: 512
  validation_batch_size: 512
  weight_decay: 0.01
  clip_grad_value: 1  # Default value for grad_value clipping
  use_grad_value_clipping: false  # Flag to enable grad_value clipping
  patience: 2
  max_training_steps: 3800 # Steps per epoch = 38960 / 1024 â‰ˆ 38
  use_lr_scheduler: true
  lr_scheduler_type: "linear_warmup"  # can be "cosine", "linear", "linear_warmup", "one_cycle", "reduce_on_plateau", or "null"
  num_dataloader_workers: 0
  
data_config:
  save_dir: "./data/labs"
  dl_reps_dir: "data/labs/DL_reps"
  dataset_path: null
  max_seq_len: 95712
  subsequence_sampling_strategy: to_end
  min_seq_len: 12
  train_subset_size: "FULL"
  train_subset_seed: null
  task_df_name: "a1c_greater_than_7"
  seq_padding_side: right
  do_include_subject_id: False
  do_include_start_time_min: False
  do_include_subsequence_indices: False

trainer_config:
  precision: "16-mixed"
  detect_anomaly: false
  default_root_dir: "${save_dir}/model_checkpoints"
  log_every_n_steps: 10
  num_nodes: 1
  sync_batchnorm: True,  # Synchronize batch normalization between GPUs
  accumulate_grad_batches: 2

experiment_dir: "./experiments"

wandb_logger_kwargs:
  name: "transformer_labs"
  project: "diabetes_labs"
  log_model: false

wandb_experiment_config_kwargs:
  entity: "jvpoulos"
  project: "diabetes_labs"
  log_model: false

do_final_validation_on_metrics: true
do_use_filesystem_sharing: false
