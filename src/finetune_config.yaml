sweep: false
use_labs: true
do_overwrite: false  
seed: 42
save_dir: "./experiments/finetune/${now:%Y-%m-%d_%H-%M-%S}"
dataset_path: null
pretrained_weights_fp: null
config:
  _target_: EventStream.transformer.config.StructuredTransformerConfig
  output_scores: false
  use_gradient_checkpointing: false
  use_cache: true # must be false if use_gradient_checkpointing is true
  use_labs: true
  use_batch_norm: true
  use_layer_norm: true
  use_flash_attention: true  # Set to false if you don't want to use Flash Attention
  problem_type: "single_label_classification"
  num_labels: 2
  do_use_sinusoidal: true # When True, it uses the LearnableFrequencySinusoidalTemporalPositionEncoding; When False, it uses the standard TemporalPositionEncoding
  do_split_embeddings: true
  categorical_embedding_dim: 32
  numerical_embedding_dim: 16
  static_embedding_mode: sum_all
  categorical_embedding_weight: 0.3
  numerical_embedding_weight: 0.7
  static_embedding_weight: 0.7
  dynamic_embedding_weight: 0.5
  do_normalize_by_measurement_index: false
  structured_event_processing_mode: conditionally_independent
  num_hidden_layers: 4
  seq_attention_types: ["global", "local"]
  seq_window_size: 171
  head_dim: 64
  num_attention_heads: 8
  max_grad_norm: 1
  intermediate_dropout: 0.3
  attention_dropout: 0.1
  input_dropout: 0.1
  resid_dropout: 0.1
  hidden_size: 512  # consistent with head_dim and num_attention_heads
  intermediate_size: 256
  layer_norm_epsilon: 1.86525967211174E-05
  task_specific_params:
    pooling_method: mean

optimization_config:
  init_lr: 0.01  # Initial learning rate
  end_lr: 0.0001   # Final learning rate
  end_lr_frac_of_init_lr: 0.01  # This should be equal to end_lr / init_lr
  max_epochs: 300 
  batch_size: 512
  validation_batch_size: 512
  lr_frac_warmup_steps: 0.05
  lr_decay_power: 0.05
  weight_decay: 0.01
  clip_grad_value: 1  # Default value for grad_value clipping
  use_grad_value_clipping: true  # Flag to enable grad_value clipping
  patience: 10
  lr_num_warmup_steps: 200  
  max_training_steps: 316500 # steps per epoch = training samples/effective batch size
  use_lr_scheduler: true
  lr_scheduler_type: "linear"  # can be "cosine", "linear", "one_cycle", "reduce_on_plateau", or "null"
  num_dataloader_workers: 0
  
data_config:
  save_dir: "./data/labs"
  dl_reps_dir: "data/labs/DL_reps"
  dataset_path: null
  max_seq_len: 390
  subsequence_sampling_strategy: to_end
  min_seq_len: 12
  train_subset_size: "FULL"
  train_subset_seed: null
  task_df_name: "a1c_greater_than_7"
  seq_padding_side: right
  do_include_subject_id: False
  do_include_start_time_min: False
  do_include_subsequence_indices: False

trainer_config:
  accelerator: auto
  devices: 1
  precision: "16-mixed"
  detect_anomaly: false
  log_every_n_steps: 10
  strategy: "ddp_find_unused_parameters_true"
  accumulate_grad_batches: 2

experiment_dir: "./experiments"

wandb_logger_kwargs:
  name: "transformer_labs"
  project: "diabetes_finetune_labs"
  team: null
  log_model: false
  do_log_graph: false

wandb_experiment_config_kwargs:
  entity: "jvpoulos"
  project: "diabetes_finetune_labs"
  team: null
  log_model: false
  do_log_graph: false
  
do_final_validation_on_metrics: false
do_use_filesystem_sharing: false
