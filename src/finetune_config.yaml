do_overwrite: false  
seed: 42
save_dir: "./experiments/finetune/${now:%Y-%m-%d_%H-%M-%S}"
dataset_path: null
config:
  _target_: EventStream.transformer.config.StructuredTransformerConfig
  use_batch_norm: false
  use_layer_norm: true
  use_flash_attention: true  # Set to false if you don't want to use Flash Attention
  problem_type: "single_label_classification"
  num_labels: 2
  do_use_learnable_sinusoidal_ATE: true
  do_split_embeddings: true
  categorical_embedding_dim: 64
  numerical_embedding_dim: 64
  static_embedding_mode: sum_all
  categorical_embedding_weight: 0.3
  numerical_embedding_weight: 0.5
  static_embedding_weight: 0.4
  dynamic_embedding_weight: 0.5
  do_normalize_by_measurement_index: false
  structured_event_processing_mode: conditionally_independent
  num_hidden_layers: 6
  seq_attention_types: ["global", "local"]
  seq_window_size: 168
  head_dim: 32
  num_attention_heads: 8
  max_grad_norm: 1
  intermediate_dropout: 0.3
  attention_dropout: 0.3
  input_dropout: 0.3
  resid_dropout: 0.3
  hidden_size: 256  # 32*8 = 256, consistent with head_dim and num_attention_heads
  intermediate_size: 256
  task_specific_params:
    pooling_method: mean

optimization_config:
  init_lr: 1e-3  # Initial learning rate
  end_lr: 1e-5   # Final learning rate
  end_lr_frac_of_init_lr: 0.01  # This should be equal to end_lr / init_lr
  max_epochs: 100 
  batch_size: 256
  validation_batch_size: 256
  lr_frac_warmup_steps: 0.05
  lr_decay_power: 0.01
  weight_decay: 0.01
  clip_grad_value: 1  # Default value for grad_value clipping
  use_grad_value_clipping: True  # Flag to enable grad_value clipping
  patience: 2
  gradient_accumulation: 2
  num_dataloader_workers: 13
  lr_num_warmup_steps: 200  
  max_training_steps: 316500 # steps per epoch = training samples/effective batch size
  use_lr_scheduler: true
  lr_scheduler_type: "linear"  # can be "cosine", "linear", "one_cycle", "reduce_on_plateau"
  
data_config:
  save_dir: "./data"
  dl_reps_dir: "data/DL_reps"
  dataset_path: null
  max_seq_len: 256
  subsequence_sampling_strategy: to_end
  min_seq_len: 4
  train_subset_size: "FULL"
  train_subset_seed: null
  task_df_name: "a1c_greater_than_7"
  seq_padding_side: right
  do_include_subject_id: False
  do_include_start_time_min: False
  do_include_subsequence_indices: False

trainer_config:
  accelerator: auto
  devices: 3
  precision: "16-mixed"
  detect_anomaly: false
  log_every_n_steps: 100
  strategy: "ddp_find_unused_parameters_true"

experiment_dir: "./experiments"

wandb_logger_kwargs:
  name: "jvpoulos"
  project: "diabetes_finetune"
  team: null
  log_model: false
  do_log_graph: false

wandb_experiment_config_kwargs:
  entity: "finetune"
  project: "diabetes_finetune"
  team: null
  log_model: false
  do_log_graph: false
  
do_final_validation_on_metrics: false
do_use_filesystem_sharing: false
do_debug_mode: false
