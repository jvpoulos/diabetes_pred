sweep: false
use_labs: true
do_overwrite: false  
seed: 42
save_dir: "./experiments/finetune/${now:%Y-%m-%d_%H-%M-%S}"
dataset_path: null
pretrained_weights_fp: None
#pretrained_weights_fp: "./experiments/finetune/2024-10-21_09-31-04/checkpoints/last.ckpt"

config:
  _target_: EventStream.transformer.config.StructuredTransformerConfig
  activation_function: 'gelu'
  vocab_size: 33804
  use_fake_feature: true
  fake_feature_correlation: 0.8
  use_gradient_checkpointing: true
  use_cache: false # must be false if use_gradient_checkpointing is true
  use_labs: true
  use_static_features: true
  use_batch_norm: true
  use_layer_norm: true
  use_flash_attention: false
  output_attentions: true
  problem_type: "single_label_classification"
  num_labels: 2
  do_use_sinusoidal: true # When True, it uses the LearnableFrequencySinusoidalTemporalPositionEncoding; When False, it uses the standard TemporalPositionEncoding
  do_split_embeddings: true
  categorical_embedding_dim: 32
  numerical_embedding_dim: 16
  static_embedding_mode: sum_all
  categorical_embedding_weight: 0.3
  numerical_embedding_weight: 0.7
  static_embedding_weight: 0.7
  dynamic_embedding_weight: 0.5
  do_normalize_by_measurement_index: false
  structured_event_processing_mode: conditionally_independent
  num_hidden_layers: 3
  seq_attention_types: ["global", "local"]
  seq_window_size: 168 # An integer specifying the size of the attention window.
  head_dim: 64
  max_position_embeddings: 1024
  num_attention_heads: 2
  max_grad_norm: 1
  intermediate_dropout: 0.3
  attention_dropout: 0.1
  input_dropout: 0.1
  resid_dropout: 0.1
  hidden_size: 128  # consistent with head_dim and num_attention_heads
  intermediate_size: 128
  layer_norm_epsilon: 1e-3
  task_specific_params:
    pooling_method: mean

optimization_config:
  init_lr: 0.01  # Initial learning rate
  end_lr: 0.0001   # Final learning rate
  end_lr_frac_of_init_lr: 0.01  # This should be equal to end_lr / init_lr
  max_epochs: 10
  batch_size: 128
  validation_batch_size: 128
  weight_decay: 0.01
  patience: 2
  max_training_steps: 760 # Steps per epoch = 38960 / 512 â‰ˆ 76
  use_lr_scheduler: true
  lr_scheduler_type: "linear"  # can be "cosine", "linear", "linear_warmup", "one_cycle", "reduce_on_plateau", or "null"
  num_dataloader_workers: 0
  
data_config:
  save_dir: "./data/labs"
  dl_reps_dir: "data/labs/DL_reps"
  dataset_path: null
  max_seq_len: 95712
  subsequence_sampling_strategy: to_end
  min_seq_len: 12
  train_subset_size: "FULL"
  train_subset_seed: null
  task_df_name: "a1c_greater_than_7"
  seq_padding_side: right
  do_include_subject_id: False
  do_include_start_time_min: False
  do_include_subsequence_indices: False

trainer_config:
  accelerator: "gpu"
  devices: 3
  strategy: "ddp_find_unused_parameters_true"
  detect_anomaly: false
  default_root_dir: "${save_dir}/model_checkpoints"
  log_every_n_steps: 100
  accumulate_grad_batches: 4
  precision: "32"
  gradient_clip_val: 1.0
  deterministic: false  # Set to true only if exact reproducibility is needed
  benchmark: true  # Can improve performance
  sync_batchnorm: true  # Important for multi-GPU training
  
experiment_dir: "./experiments"

wandb_logger_kwargs:
  name: "transformer_labs"
  project: "diabetes_labs"

wandb_experiment_config_kwargs:
  entity: "jvpoulos"
  project: "diabetes_labs"

do_final_validation_on_metrics: true
do_use_filesystem_sharing: false
